---
layout: post
title: A Style-Based Generator Architecture for GAN (StyleGAN)
categories: Group-Meeting
description: des about stylegan
keywords: stylegan
---

연구실 그룹 미팅에서 발표한 논문을 바탕으로 나중에 보려고 정리한 글입니다.
오늘 주제는 Style GAN입니다.
자세한 제목은 A Style-Based Generator Architecture for GAN 입니다.

### 1. Structure

![Structure](/images/posts/0416/0416-1.png)

Style GAN의 전체 구조입니다.
(a) Traditional 에서는 Latent vector z를 그대로 이미지 Decoder에 통과시켜서 이미지를 얻습니다.

그에 반면 StyleGAN에서 주장하는 Style-based generator에서는 Latent z를 normalize를 시켜서

Mapping Network(8 FC)를 한번 거쳐서 Intermediate vecotr w를 얻은 다음 Synthesis Network에서 이미지를 생성하는 모습압니다.

여기서 w는 Style Transfer 논문을 읽어야 좀 자세하게 알 것 같은데 생성하는 이미지에서의 Style을 결정하는 변수라고 생각됩니다.

#### a. Mapping Network

![Mapping](/images/posts/0416/0416-2.png)

여기서 논문에 핵심 용어 중 하나인 Disentanglement를 설명하면,

이미지의 중요한 특징들이 w vector 값 하나로 매핑되어 잘 변하면 Disentanglement라고 하고,

반대로 w의 값 하나만 바뀌어도 생성되는 전체적인 이미지가 다 바뀌어 버린다면 이것을 Entanglement라고 합니다.

예를 들어, w = [0,0,0,0,0,0] 이었을 때, 한 사람의 얼굴의 사진이 생성되었다고 가정합시다. 물론 말은 안되지만요.

근데 여기서 w = [1,0,0,0,0,0] 으로 값이 바뀌었을 때, 이 사람의 머리카락 색깔만 바뀌었다고 이 모델은 Disentanglement가 잘 이루어져있다는 것을 뜻합니다.

Mapping Network에서는 단순히 Random vector z를 뽑았던 기존 네트워크를 개선시켜, 이를 잘 매핑시키기 위하여 Mapping Network를 하나 더 둔겁니다.

#### b. Synthesis Network

![Synthesis](/images/posts/0416/0416-3.png)

*출처: Rani Horev’s blog*

이 그림에서 보면 w도 넣어주고 진짜 noise도 synthesis network에 들어가는 것을 볼 수 있습니다.

그리고 특이한 것은 그림에는 나와있지 않지만, synthesis network은 시작할 때 constant vector로 시작합니다.

원래는 앞의 tranditional 버전을 보면 z를 직접 넣어서 시작하는 데 여기서는 style를 랜덤하게 채우기 때문에 이 부분에서는 constant를 활용하는 모습입니다.

그리고 이 vector들을 연산할 때 AdaIN 기법을 쓰는데, 이것도 또 하나의 논문이라서 다음에 이 내용을 정리해서 올리겠습니다.

쉽게 생각하면 AdaIN은 Style 관련 Model에서 쓰는 Batch Norm 이라고 생각하면 이해하기 쉬운 것 같습니다.

그리고 정확히 몇 개인지는 세기는 귀찮지만, 처음 이미지 생성할 때는 4 X 4를 생성하고 그 다음부터 계속 upsampling을 거쳐서 끝에는 1024 X 1024 같은 대형 이미지까지 얻는 것을 볼 수 있습니다.

#### c. Result

논문 전체적인 구조는 여기까지이고, 결과는 아래처럼 나온다고 하네요.

![Result](/images/posts/0416/0416-4.png)


### 2. Training Tech

StyleGAN을 학습시킬 때 쓰는 잡기술을 소개합니다.

#### a. Style mixing

쉽게 생각하면 우리가 Image Classification을 할 때, Data Augmentation을 통하여 Model Regularization을 꾀하는 것처럼

여기서는 Style Mixing이라는 방법으로 Model Regularization을 노립니다.

![Result](/images/posts/0416/0416-5.png)

*출처: Rani Horev’s blog*

방법은 Synthesis Network에서 w를 이미지 생성할 때마다 넣어주는데, 이를 생성하는 z1, z2를 미리 세팅합니다.

그리고, mapping network를 통과시키면 w1, w2가 생기겠죠. 이걸 논문에서는  &lambda;(lambda)로 표현하는데,

만약, &lambda;가 0.7이라면 전체 w를 10번 synthesis network에 넣는다면, 이중 7번은 w1, 3번은 w2를 synthesis network에 반영하는 것입니다.

그러면 또 그거에 따른 이미지가 생성되겠죠.

#### b. Truncation Trick

이거를 어렵게 생각했었는데, 그렇게 어려운 개념은 아니었습니다.

이전에 나온 GAN 논문을 읽지 못하고, 처음 읽은 GAN논문이 styleGAN이라서 좀 어렵게 느껴졌는데,

처음에 z를 생성할 때, 평균적인 z에서 멀리 떨어져있는 z를 날려버리는 잡기술입니다.

원래의 GAN에서는 z를 날리는데, StyleGAN에서는 z가 아니라 z와 mapping network를 통해서 만드는 w에 truncation trick을 사용합니다.

![Truncation Trick](/images/posts/0416/0416-6.png)


### 3. Measure Feature Disentenglement

앞에서 설명한 Disentenglement가 잘 이루어지고 있느냐를 정확하게 어떻게 측정할 것인지에 대한 내용입니다.

이 논문에서는 총 2개를 제안하고 있습니다.

#### a. Perceptual Path Length

이 내용은 좀 어려웠는데요.
기본 내용은 noise역할을 하는 z의 값이 살짝 바뀌면 이미지가 많이 바뀌냐 를 측정하는 겁니다.

수식적으로 보면 t에다가 &epsilon;(epsilon)을 살짝 더합니다. &epsilon; 값은 대략 10의 -4승정도 됩니다. 논문에서 이렇게 본 것 같아요.

이를 z에 대해 계산할 때는 z는 애초에 distribution이 정해져 있기 때문에 Slerp(spherical interpoloation) 원형 보간법이라고 해야하나.. 어쨋든 이 연산을 하고
w는 그냥 lerp를 진행하게 됩니다.

![slerp](/images/posts/0416/0416-7.png){: width="300"}
![lerp](/images/posts/0416/0416-8.png){: width="300"}

그러면 결과가 이렇게 나옵니다.

![result](/images/posts/0416/0416-9.png)

그러면 여기서 또 궁금한 것이 full 은 뭐고, end path length는 또 머냐라고 할 수 있는데

여기서 t는 얼마 만큼의 비율을 줄 것이냐 인데, 아래 그림을 보면 이해가 좀 더 쉬울 것 같습니다.

![result](/images/posts/0416/0416-10.png)

*출처: Jaeyun's blog*

그래서 이 t를 0,1 즉 아예 하나만 가지고 계산하는 path length가 end path length인 것이죠.
그리고 full은 0부터 1사이의 값 중에 랜덤으로 하나 정해서 계산합니다.

![result](/images/posts/0416/0416-11.png){: width="300"}

이렇게 다르게 계산하는 이유는 w에다가 path length를 계산할 때 interpolation을 수행하면 (w 나 f(z)나 같은 말)

interpolation한 w가 training space에 존재하지 않아 path length가 더 커질 수 있다고 합니다.

앞에 있는 결과 표를 보면 w를 게산했을 때, full에 end보다 큰 것을 확인할 수 있습니다.

#### b. Linear Separability

1. 이거는 먼저 CELEBA-HQ 데이터 셋을 기반으로 이미지를 200,000개 만듭니다.

사실 데이터 셋을 무엇을 쓰냐는 크게 중요하지 않은 것 같습니다.

2. 200,000 개 중에 이상하게 만들어진(least confidennt) 이미지 100,000개를 날립니다. 즉, 절반을 날립니다.

3. 그리고 남아있는 100,000개 이미지를 pre-trained 된 auxiliary model에 binary classification을 수행합니다. 예를 들면, 남자 이미지냐 여자 이미지냐 이런 것들을 분류하는 겁니다. 그리고 이 model로 이미지에 pseudo-labelling을 거칩니다.

4. 그 다음으로 각각의 이미지에 매핑되어 있는 w(latent-space point)를 기준으로 Linear SVM을 수행합니다. 물론 3번에서 설명한 labeling을 기준으로 말이죠.

5. 그리고 SVM을 학습시키고 나온 Conditional entropy를 계산한 값을 linear separability라고 합니다.

Conditional Entropy : H(Y &#124; X)

X : classes predicted by the SVM


Y : classes determined by pre-trained classifier 

### 4. Conclusion

Style Transfer 기법을 처음 접했을 때, 이걸 어디다가 쓰지? 하는 생각이 가장 먼저 들었는데 이를 GAN에 접목시켜서 놀라운 결과를 얻은 게 신기합니다.

Style기반으로 만들 듯이 다른 분야의 논문을 GAN과 잘 접목시키면 또 다른 놀라운 결과를 만들 수 있지 않을까 생각됩니다.

혹시 틀린 내용이 있을 수도 있습니다!

### 5. Reference

[Paper](https://arxiv.org/abs/1812.04948)

#### Image Reference

[Rani Horev’s blog](https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431)

[Jaeyun's blog](https://jayhey.github.io/deep%20learning/2019/01/16/style_based_GAN_2/)


